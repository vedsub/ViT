{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM+SgQ0EvSJ0Fh4jaXsrhXa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vedsub/ViT/blob/main/ViT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6Ng_JgBvbyp",
        "outputId": "38801f9e-5664-47ec-a9a1-6afb86520a1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import random\n",
        "import timeit\n",
        "import math\n",
        "import numpy\n",
        "import numpy as np\n",
        "from torch import optim\n",
        "from torch import nn\n",
        "!pip install torchinfo\n",
        "from torchinfo import summary\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from dataclasses import dataclass\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class vit_config:\n",
        "    num_channels: int = 3\n",
        "    batch_size:int = 16\n",
        "    image_size: int = 224\n",
        "    patch_size: int = 16\n",
        "    num_heads:int = 8\n",
        "    dropout: float = 0.0\n",
        "    layer_norm_eps: float = 1e-6\n",
        "    num_encoder_layers: int = 12\n",
        "    random_seed: int = 42\n",
        "    epochs: int = 30\n",
        "    num_classes: int = 10\n",
        "    learning_rate: float = 1e-5\n",
        "    adam_weight_decay: int = 0\n",
        "    adam_betas: tuple = (0.9, 0.999)\n",
        "    embd_dim: int = (patch_size ** 2) * num_channels           # 768\n",
        "    num_patches: int = (image_size // patch_size) ** 2         # 196\n",
        "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Z4BWBM5wz-s",
        "outputId": "9a0620ec-0368-4fcf-85ac-45b031aeb3f4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = vit_config\n",
        "\n",
        "random.seed(config.random_seed)\n",
        "numpy.random.seed(config.random_seed)\n",
        "torch.manual_seed(config.random_seed)\n",
        "torch.cuda.manual_seed(config.random_seed)\n",
        "torch.cuda.manual_seed_all(config.random_seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = True"
      ],
      "metadata": {
        "id": "PLEoLKIRycoW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#a custom embedding layer that concatenates and returns the patch embeddings along with position embeddings for each patch of the input image."
      ],
      "metadata": {
        "id": "ISbEuEpdAl_s"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VisionEmbedding(nn.Module):\n",
        "    def __init__(self, config: vit_config):\n",
        "        super().__init__()\n",
        "\n",
        "        self.config  = config\n",
        "        self.patch_embedding = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=config.num_channels,\n",
        "                out_channels=config.embd_dim,\n",
        "                kernel_size=config.patch_size,\n",
        "                stride=config.patch_size,\n",
        "                padding=\"valid\"\n",
        "            ),\n",
        "            nn.Flatten(start_dim=2)\n",
        "        )\n",
        "\n",
        "        self.cls_token = nn.Parameter(torch.randn(size=(1, 1, config.embd_dim)), requires_grad=True)\n",
        "        self.pos_embeddings = nn.Parameter(torch.randn(size=(1, config.num_patches + 1, config.embd_dim)), requires_grad=True)\n",
        "        self.dropout = nn.Dropout(p=config.dropout)\n",
        "\n",
        "    def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
        "        cls_token = self.cls_token.expand(x.shape[0], -1, -1)\n",
        "\n",
        "        patch_embd = self.patch_embedding(x).transpose(2,1)\n",
        "        patch_embd = torch.cat([cls_token, patch_embd], dim=1)\n",
        "        embd = self.pos_embeddings + patch_embd\n",
        "        embd = self.dropout(embd)\n",
        "        return embd"
      ],
      "metadata": {
        "id": "hgaliYxAA4Oy"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84cf8718",
        "outputId": "d95d59a5-f56a-4b30-defc-129f5b3d2f73"
      },
      "source": [
        "model = VisionEmbedding(config)\n",
        "dummy_input = torch.randn(1, config.num_channels, config.image_size, config.image_size).to(config.device)\n",
        "output = model(dummy_input)\n",
        "print(f\"Output shape: {output.shape}\")\n",
        "print(f\"Output (first 5 elements): {output[0, :5, :5]}\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([1, 197, 768])\n",
            "Output (first 5 elements): tensor([[-0.4828, -2.9331, -1.0430,  0.5191,  1.1593],\n",
            "        [-0.5916,  0.7174, -0.8135, -0.0548,  0.7490],\n",
            "        [-0.5815,  0.2315, -0.9726, -1.6018,  0.6856],\n",
            "        [-0.2447,  0.3341,  0.1944, -0.9270,  0.7581],\n",
            "        [-0.9925,  0.5160,  0.9445,  0.3006,  0.7511]],\n",
            "       grad_fn=<SliceBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class VisionAttention(nn.Module):\n",
        "    def __init__(self, config:vit_config):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embd_dim = config.embd_dim\n",
        "        self.num_heads = config.num_heads\n",
        "        self.dropout = config.dropout\n",
        "\n",
        "        self.q_proj = nn.Linear(self.embd_dim, self.embd_dim)\n",
        "        self.k_proj = nn.Linear(self.embd_dim, self.embd_dim)\n",
        "        self.v_proj = nn.Linear(self.embd_dim, self.embd_dim)\n",
        "        self.out_proj = nn.Linear(self.embd_dim, self.embd_dim)\n",
        "\n",
        "    def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
        "        B, T, C = x.shape\n",
        "\n",
        "        query = self.q_proj(x)\n",
        "        key = self.k_proj(x)\n",
        "        value = self.v_proj(x)\n",
        "\n",
        "        query = query.view(B, T, self.num_heads, C//self.num_heads).transpose(1,2)\n",
        "        key = key.view(B, T, self.num_heads, C//self.num_heads).transpose(1,2)\n",
        "        value = value.view(B, T, self.num_heads, C//self.num_heads).transpose(1,2)\n",
        "\n",
        "        attn_score = (query @ key.transpose(-2, -1)) * (1.0 / math.sqrt(key.size(-1)))\n",
        "        attn_score = F.softmax(attn_score, dim=-1).to(query.dtype)\n",
        "\n",
        "        attn_out = (attn_score @ value).transpose(1,2)\n",
        "        attn_out = attn_out.reshape(B, T, C).contiguous()\n",
        "        attn_out = self.out_proj(attn_out)\n",
        "        attn_out = F.dropout(attn_out, p=self.dropout, training=self.training)\n",
        "\n",
        "        return attn_out"
      ],
      "metadata": {
        "id": "XhOuMs6eHFPA"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VisionMLP(nn.Module):\n",
        "    def __init__(self, config:vit_config):\n",
        "        super().__init__()\n",
        "\n",
        "        self.layer1 = nn.Linear(config.embd_dim, 3 * config.embd_dim)\n",
        "        self.layer2 = nn.Linear(3 * config.embd_dim, config.embd_dim)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.layer1(x)\n",
        "        x = nn.functional.gelu(x, approximate=\"tanh\")\n",
        "        x = self.layer2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "BQSGZM9xn7kS"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VisionEncoderLayer(nn.Module):\n",
        "    def __init__(self, config: vit_config):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embd_dim = config.embd_dim\n",
        "        self.attn = VisionAttention(config)\n",
        "        self.layer_norm1 = nn.LayerNorm(self.embd_dim, eps=config.layer_norm_eps)\n",
        "        self.mlp = VisionMLP(config)\n",
        "        self.layer_norm2 = nn.LayerNorm(self.embd_dim, eps=config.layer_norm_eps)\n",
        "\n",
        "    def forward(self, x:torch.Tensor) -> torch.Tensor:\n",
        "        x = x + self.attn(self.layer_norm1(x))\n",
        "        x = x + self.mlp(self.layer_norm2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "EWzXlm1gc40F"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VisionEncoder(nn.Module):\n",
        "    def __init__(self, config: vit_config):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([VisionEncoderLayer(config) for _ in range(config.num_encoder_layers)])\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "n8qUpByIc6Cy"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, config= vit_config):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = VisionEmbedding(config)\n",
        "        self.encoder = VisionEncoder(config)\n",
        "        self.mlp_head = nn.Sequential(\n",
        "            nn.LayerNorm(config.embd_dim, eps=config.layer_norm_eps),\n",
        "            nn.Linear(config.embd_dim, config.num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x:torch.Tensor ) -> torch.Tensor:\n",
        "        x = self.embedding(x)\n",
        "        x = self.encoder(x)\n",
        "        x = self.mlp_head(x[:, 0, :])\n",
        "        return x"
      ],
      "metadata": {
        "id": "S_Igr-8Gc_MX"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vit = VisionTransformer(vit_config)\n",
        "summary(model=vit,\n",
        "        input_size=(16, 3, 224, 224),\n",
        "        col_names= [\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "        col_width=20,\n",
        "        row_settings= [\"var_names\"]\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1j2bnKOsdcCu",
        "outputId": "189e5ee1-2ee4-4df3-e4f3-1fbe128bcadd"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "=============================================================================================================================\n",
              "Layer (type (var_name))                       Input Shape          Output Shape         Param #              Trainable\n",
              "=============================================================================================================================\n",
              "VisionTransformer (VisionTransformer)         [16, 3, 224, 224]    [16, 10]             --                   True\n",
              "├─VisionEmbedding (embedding)                 [16, 3, 224, 224]    [16, 197, 768]       152,064              True\n",
              "│    └─Sequential (patch_embedding)           [16, 3, 224, 224]    [16, 768, 196]       --                   True\n",
              "│    │    └─Conv2d (0)                        [16, 3, 224, 224]    [16, 768, 14, 14]    590,592              True\n",
              "│    │    └─Flatten (1)                       [16, 768, 14, 14]    [16, 768, 196]       --                   --\n",
              "│    └─Dropout (dropout)                      [16, 197, 768]       [16, 197, 768]       --                   --\n",
              "├─VisionEncoder (encoder)                     [16, 197, 768]       [16, 197, 768]       --                   True\n",
              "│    └─ModuleList (layers)                    --                   --                   --                   True\n",
              "│    │    └─VisionEncoderLayer (0)            [16, 197, 768]       [16, 197, 768]       5,907,456            True\n",
              "│    │    └─VisionEncoderLayer (1)            [16, 197, 768]       [16, 197, 768]       5,907,456            True\n",
              "│    │    └─VisionEncoderLayer (2)            [16, 197, 768]       [16, 197, 768]       5,907,456            True\n",
              "│    │    └─VisionEncoderLayer (3)            [16, 197, 768]       [16, 197, 768]       5,907,456            True\n",
              "│    │    └─VisionEncoderLayer (4)            [16, 197, 768]       [16, 197, 768]       5,907,456            True\n",
              "│    │    └─VisionEncoderLayer (5)            [16, 197, 768]       [16, 197, 768]       5,907,456            True\n",
              "│    │    └─VisionEncoderLayer (6)            [16, 197, 768]       [16, 197, 768]       5,907,456            True\n",
              "│    │    └─VisionEncoderLayer (7)            [16, 197, 768]       [16, 197, 768]       5,907,456            True\n",
              "│    │    └─VisionEncoderLayer (8)            [16, 197, 768]       [16, 197, 768]       5,907,456            True\n",
              "│    │    └─VisionEncoderLayer (9)            [16, 197, 768]       [16, 197, 768]       5,907,456            True\n",
              "│    │    └─VisionEncoderLayer (10)           [16, 197, 768]       [16, 197, 768]       5,907,456            True\n",
              "│    │    └─VisionEncoderLayer (11)           [16, 197, 768]       [16, 197, 768]       5,907,456            True\n",
              "├─Sequential (mlp_head)                       [16, 768]            [16, 10]             --                   True\n",
              "│    └─LayerNorm (0)                          [16, 768]            [16, 768]            1,536                True\n",
              "│    └─Linear (1)                             [16, 768]            [16, 10]             7,690                True\n",
              "=============================================================================================================================\n",
              "Total params: 71,641,354\n",
              "Trainable params: 71,641,354\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.GIGABYTES): 2.99\n",
              "=============================================================================================================================\n",
              "Input size (MB): 9.63\n",
              "Forward/backward pass size (MB): 2343.27\n",
              "Params size (MB): 285.96\n",
              "Estimated Total Size (MB): 2638.86\n",
              "============================================================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    }
  ]
}