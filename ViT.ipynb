{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6Ng_JgBvbyp",
        "outputId": "286c1838-02a6-4f58-c792-5d82136d6bb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import random\n",
        "import timeit\n",
        "import math\n",
        "import numpy\n",
        "import numpy as np\n",
        "from torch import optim\n",
        "from torch import nn\n",
        "!pip install torchinfo\n",
        "from torchinfo import summary\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from dataclasses import dataclass\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class vit_config:\n",
        "    num_channels: int = 3\n",
        "    batch_size:int = 16\n",
        "    image_size: int = 224\n",
        "    patch_size: int = 16\n",
        "    num_heads:int = 8\n",
        "    dropout: float = 0.0\n",
        "    layer_norm_eps: float = 1e-6\n",
        "    num_encoder_layers: int = 12\n",
        "    random_seed: int = 42\n",
        "    epochs: int = 30\n",
        "    num_classes: int = 10\n",
        "    learning_rate: float = 1e-5\n",
        "    adam_weight_decay: int = 0\n",
        "    adam_betas: tuple = (0.9, 0.999)\n",
        "    embd_dim: int = (patch_size ** 2) * num_channels           # 768\n",
        "    num_patches: int = (image_size // patch_size) ** 2         # 196\n",
        "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Z4BWBM5wz-s",
        "outputId": "4b9a80eb-5523-4b99-b707-512c0454ae4e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = vit_config\n",
        "\n",
        "random.seed(config.random_seed)\n",
        "numpy.random.seed(config.random_seed)\n",
        "torch.manual_seed(config.random_seed)\n",
        "torch.cuda.manual_seed(config.random_seed)\n",
        "torch.cuda.manual_seed_all(config.random_seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = True"
      ],
      "metadata": {
        "id": "PLEoLKIRycoW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#a custom embedding layer that concatenates and returns the patch embeddings along with position embeddings for each patch of the input image."
      ],
      "metadata": {
        "id": "ISbEuEpdAl_s"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VisionEmbedding(nn.Module):\n",
        "    def __init__(self, config: vit_config):\n",
        "        super().__init__()\n",
        "\n",
        "        self.config  = config\n",
        "        self.patch_embedding = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=config.num_channels,\n",
        "                out_channels=config.embd_dim,\n",
        "                kernel_size=config.patch_size,\n",
        "                stride=config.patch_size,\n",
        "                padding=\"valid\"\n",
        "            ),\n",
        "            nn.Flatten(start_dim=2)\n",
        "        )\n",
        "\n",
        "        self.cls_token = nn.Parameter(torch.randn(size=(1, 1, config.embd_dim)), requires_grad=True)\n",
        "        self.pos_embeddings = nn.Parameter(torch.randn(size=(1, config.num_patches + 1, config.embd_dim)), requires_grad=True)\n",
        "        self.dropout = nn.Dropout(p=config.dropout)\n",
        "\n",
        "    def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
        "        cls_token = self.cls_token.expand(x.shape[0], -1, -1)\n",
        "\n",
        "        patch_embd = self.patch_embedding(x).transpose(2,1)\n",
        "        patch_embd = torch.cat([cls_token, patch_embd], dim=1)\n",
        "        embd = self.pos_embeddings + patch_embd\n",
        "        embd = self.dropout(embd)\n",
        "        return embd"
      ],
      "metadata": {
        "id": "hgaliYxAA4Oy"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84cf8718",
        "outputId": "fef705fe-6813-459f-e3ce-ff4c2e66fc44"
      },
      "source": [
        "model = VisionEmbedding(config)\n",
        "dummy_input = torch.randn(1, config.num_channels, config.image_size, config.image_size).to(config.device)\n",
        "output = model(dummy_input)\n",
        "print(f\"Output shape: {output.shape}\")\n",
        "print(f\"Output (first 5 elements): {output[0, :5, :5]}\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([1, 197, 768])\n",
            "Output (first 5 elements): tensor([[-0.4828, -2.9331, -1.0430,  0.5191,  1.1593],\n",
            "        [-0.5916,  0.7174, -0.8135, -0.0548,  0.7490],\n",
            "        [-0.5815,  0.2315, -0.9726, -1.6018,  0.6856],\n",
            "        [-0.2447,  0.3341,  0.1944, -0.9270,  0.7581],\n",
            "        [-0.9925,  0.5160,  0.9445,  0.3006,  0.7511]],\n",
            "       grad_fn=<SliceBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XhOuMs6eHFPA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}